from __future__ import annotations

from pathlib import Path

from dotenv import load_dotenv

load_dotenv(dotenv_path=Path(__file__).with_name(".env"))

import base64
import uuid

import pandas as pd
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage

from src.agent.graph import create_graph
from src.agent.models import ExecResult, ReasonDecision, ReportOutput


st.set_page_config(page_title="Data Analysis AI Agent", layout="wide")


def _init_session():
    if "app" not in st.session_state:
        st.session_state.app = create_graph()
    if "state" not in st.session_state:
        st.session_state.state = None
    # ChatGPTé¢¨ã«è¡¨ç¤ºã™ã‚‹ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆè¡¨ç¤ºç”¨ã€‚LLMç”¨ã® state["messages"] ã¨ã¯åˆ†é›¢ï¼‰
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []


def _reset():
    st.session_state.state = None
    st.session_state.chat_history = []


def _render_report(report: dict):
    ro = ReportOutput.model_validate(report)
    st.markdown("**ğŸ“Š Report Summary**")
    st.markdown(ro.summary)

    if ro.table_markdown:
        st.markdown("**ğŸ“‹ Tables**")
        for t in ro.table_markdown:
            st.markdown(t)

    if ro.plot_png_base64:
        st.markdown("**ğŸ“ˆ Plots**")
        for b64 in ro.plot_png_base64:
            st.image(base64.b64decode(b64))

    if ro.json:
        st.markdown("**ğŸ”§ JSON**")
        for j in ro.json:
            st.json(j)


_init_session()

with st.sidebar:
    st.header("Inputs")
    uploaded = st.file_uploader("Upload CSV", type=["csv"])
    if st.button("Reset session"):
        _reset()
    st.caption("env: OPENAI_API_KEY / OPENAI_MODEL")
    
    # Phase 3: Saved Models
    st.divider()
    st.subheader("ğŸ“Š Saved Models")
    from src.agent.tools.model_ops import list_saved_models
    models = list_saved_models()
    
    if models:
        for model in models[:5]:  # æœ€æ–°5ä»¶
            st.markdown(f"**{model['model_name']}**")
            st.caption(f"Type: {model['model_type']}")
            score_label = "RÂ²" if model['task_type'] == 'regression' else "Acc"
            st.caption(f"Test {score_label}: {model.get('test_score', 0):.3f}")
            st.caption(f"Created: {model['created_at'][:10]}")
    else:
        st.info("No models saved yet")

if uploaded is None:
    st.info("Upload a CSV to begin.")
    st.stop()

df = pd.read_csv(uploaded)

if "processing" not in st.session_state:
    st.session_state.processing = False

# ãƒ¡ãƒ¢ãƒªã‚’èª­ã¿è¾¼ã¿
from src.agent.memory.loader import load_memory
memories = [m.model_dump() for m in load_memory()]

if st.session_state.state is None:
    st.session_state.state = {
        "messages": [],
        "df": df,
        "memories": memories,
        "decision": None,
        "last_code": None,
        "last_exec": None,
        "report": None,
    }
else:
    # dfã¯å¸¸ã«æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å„ªå…ˆï¼ˆå˜ä¸€CSVå‰æï¼‰
    st.session_state.state["df"] = df
    # ãƒ¡ãƒ¢ãƒªã‚‚æ¯å›æœ€æ–°ã‚’èª­ã¿è¾¼ã¿
    st.session_state.state["memories"] = memories

state = st.session_state.state

# ã‚¿ãƒ–æ§‹æˆ
tab1, tab2 = st.tabs(["ï¿½ Analysis", " Prediction"])

# Tab 1: Analysis (Data Preview + Chat)
with tab1:
    # Data Previewï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆï¼‰
    with st.expander(f"ğŸ“Š Data Preview ({df.shape[0]} rows Ã— {df.shape[1]} columns)", expanded=False):
        st.dataframe(df, use_container_width=True)
    
    # Chat
    # ChatGPTé¢¨: chat_history ã‚’æ™‚ç³»åˆ—ã§è¡¨ç¤ºï¼ˆLLMç”¨messagesã¨ã¯åˆ†é›¢ï¼‰
    for e in st.session_state.get("chat_history", []):
        etype = e.get("type")
        if etype == "user":
            with st.chat_message("user"):
                st.write(e.get("text", ""))
        elif etype == "assistant":
            with st.chat_message("assistant"):
                st.write(e.get("text", ""))
        elif etype == "code":
            with st.chat_message("assistant"):
                with st.expander("ğŸ“ å®Ÿè¡Œã‚³ãƒ¼ãƒ‰", expanded=False):
                    st.code(e.get("code", ""), language="python")
        elif etype == "report":
            with st.chat_message("assistant"):
                _render_report(e["report"])

     # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã¯ chat_history å´ã«ä¸€æœ¬åŒ–ï¼ˆæ™‚ç³»åˆ—ã®ä¸­ã«æ®‹ã™ï¼‰

    # å‡¦ç†ä¸­ï¼šã‚¹ãƒ”ãƒŠãƒ¼ã‚’è¡¨ç¤ºã—ãªãŒã‚‰å®Ÿè¡Œ
    if st.session_state.processing:
        with st.chat_message("assistant"):
            with st.spinner("åˆ†æä¸­..."):
                prev_report = st.session_state.state.get("report")
                prev_last_code = st.session_state.state.get("last_code")
                prev_messages = list(st.session_state.state.get("messages", []))
                agent_result = st.session_state.app.invoke(st.session_state.state)
                st.session_state.state = agent_result

                # run_code ã§ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å±¥æ­´ã«ç©ã‚€ï¼ˆåŒä¸€å†…å®¹ã®é‡è¤‡ã¯é¿ã‘ã‚‹ï¼‰
                new_last_code = agent_result.get("last_code")
                if new_last_code and new_last_code != prev_last_code:
                    last = st.session_state.chat_history[-1] if st.session_state.chat_history else None
                    if not (last and last.get("type") == "code" and last.get("code") == new_last_code):
                        st.session_state.chat_history.append({"type": "code", "code": new_last_code})

                # ask_clarification ç­‰ã§å¢—ãˆãŸAIMessageã‚’ chat_history ã«ç©ã‚€ï¼ˆreport_summaryã‚¿ã‚°ã¯é™¤å¤–ï¼‰
                new_messages = list(agent_result.get("messages", []))
                if len(new_messages) > len(prev_messages):
                    for m in new_messages[len(prev_messages) :]:
                        if isinstance(m, AIMessage) and m.additional_kwargs.get("source") == "report_summary":
                            continue
                        if isinstance(m, AIMessage):
                            st.session_state.chat_history.append({"type": "assistant", "text": m.content})

                new_report = agent_result.get("report")
                if new_report and new_report != prev_report:
                    st.session_state.chat_history.append({"type": "report", "report": new_report})
        st.session_state.processing = False
        st.rerun()

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    user_text = st.chat_input("åˆ†æå†…å®¹ã‚’å…¥åŠ›...")
    if user_text:
        # è¡¨ç¤ºç”¨ã®å±¥æ­´ã«ç©ã‚€ï¼ˆChatGPTé¢¨ï¼‰
        st.session_state.chat_history.append({"type": "user", "text": user_text})
        st.session_state.state["messages"] = list(st.session_state.state["messages"]) + [
            HumanMessage(content=user_text)
        ]
        st.session_state.processing = True
        st.rerun()

# Tab 2: Prediction
with tab2:
    from src.agent.tools.model_ops import list_saved_models, load_model
    
    st.header("ğŸ”® Model Prediction")
    
    models = list_saved_models()
    
    if not models:
        st.info("ğŸ“­ No models available. Train a model first in the Analysis tab!")
    else:
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_names = [m['model_name'] for m in models]
        selected_name = st.selectbox("Select Model", model_names)
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        selected_model = next(m for m in models if m['model_name'] == selected_name)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Model Type", selected_model['model_type'])
        with col2:
            st.metric("Task Type", selected_model['task_type'].capitalize())
        with col3:
            score_label = "Test RÂ²" if selected_model['task_type'] == 'regression' else "Test Accuracy"
            st.metric(score_label, f"{selected_model.get('test_score', 0):.3f}")
        
        st.divider()
        
        # å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ ç”Ÿæˆ
        st.subheader("Input Features")
        
        input_values = []
        cols = st.columns(2)
        categorical_features = selected_model.get('categorical_features', [])
        categorical_mappings = selected_model.get('categorical_mappings', {})
        
        for i, feature in enumerate(selected_model['feature_names']):
            with cols[i % 2]:
                if feature in categorical_features:
                    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: æ•°å€¤å…¥åŠ› + ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆ
                    mappings = categorical_mappings.get(feature, {})
                    if mappings:
                        # JSONã¯æ•°å€¤ã‚­ãƒ¼ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ã®ã§ã€æ•´æ•°ã«æˆ»ã™
                        mappings = {int(k): v for k, v in mappings.items()}
                        help_text = ", ".join([f"{k}={v}" for k, v in sorted(mappings.items())])
                        max_val = max(mappings.keys())
                    else:
                        help_text = "Categorical feature (encoded as numbers)"
                        max_val = 10
                    
                    value = st.number_input(
                        feature,
                        min_value=0,
                        max_value=max_val,
                        value=0,
                        step=1,
                        key=f"input_{feature}",
                        help=help_text
                    )
                else:
                    # æ•°å€¤å¤‰æ•°: é€šå¸¸ã®æ•°å€¤å…¥åŠ›
                    value = st.number_input(
                        feature,
                        value=0.0,
                        key=f"input_{feature}",
                        format="%.4f"
                    )
                input_values.append(value)
        
        st.divider()
        
        # äºˆæ¸¬ãƒœã‚¿ãƒ³
        if st.button("ğŸ¯ Predict", type="primary", use_container_width=True):
            try:
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
                model, metadata = load_model(selected_model['model_id'])
                
                # äºˆæ¸¬
                prediction = model.predict([input_values])
                
                # çµæœè¡¨ç¤º
                st.success(f"**{selected_model['target_name']}**: {prediction[0]:.4f}")
                
                # è©³ç´°æƒ…å ±
                with st.expander("ğŸ“Š Prediction Details"):
                    st.write("**Input Values:**")
                    for feature, value in zip(selected_model['feature_names'], input_values):
                        st.write(f"- {feature}: {value}")
                    st.write(f"**Model**: {selected_model['model_name']}")
                    st.write(f"**Model Type**: {selected_model['model_type']}")
            except Exception as e:
                st.error(f"âŒ Prediction failed: {e}")




